{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "684e5679",
   "metadata": {},
   "source": [
    "# GPU Configuration and Diagnostics\n",
    "\n",
    "First, let's run a detailed check of our GPU setup and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c9ba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import platform\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "import time\n",
    "\n",
    "def check_gpu_config():\n",
    "    print(\"=== System Information ===\")\n",
    "    print(f\"Python version: {platform.python_version()}\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\n=== CUDA Information ===\")\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "        \n",
    "        # Get current GPU memory usage\n",
    "        print(\"\\n=== GPU Memory Usage ===\")\n",
    "        print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "        print(f\"Cached: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\")\n",
    "        \n",
    "        # Run a simple GPU benchmark\n",
    "        print(\"\\n=== GPU Benchmark ===\")\n",
    "        sizes = [(1000, 1000), (2000, 2000), (4000, 4000)]\n",
    "        for size in sizes:\n",
    "            # Clear cache\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Warm-up\n",
    "            x = torch.randn(size, device='cuda')\n",
    "            torch.matmul(x, x)\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "            # Benchmark\n",
    "            start_time = time.time()\n",
    "            x = torch.randn(size, device='cuda')\n",
    "            y = torch.matmul(x, x)\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            \n",
    "            print(f\"Matrix multiplication {size[0]}x{size[1]}: {(end_time - start_time)*1000:.2f} ms\")\n",
    "    else:\n",
    "        print(\"\\nWARNING: CUDA is not available!\")\n",
    "        print(\"Please check your GPU drivers and CUDA installation.\")\n",
    "\n",
    "# Run the diagnostics\n",
    "check_gpu_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f03ac7",
   "metadata": {},
   "source": [
    "## Optimized GPU Sequence Analysis\n",
    "\n",
    "Now let's implement an optimized version of our sequence analysis with better GPU utilization and proper memory management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a7c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_seq_to_tensor(seq, device='cuda'):\n",
    "    \"\"\"Convert a DNA sequence to one-hot encoded tensor with optimized GPU memory usage\"\"\"\n",
    "    # Pre-allocate memory on GPU\n",
    "    seq = seq.upper()\n",
    "    tensor = torch.zeros((len(seq), 4), device=device)\n",
    "    \n",
    "    # Vectorized operation for each base\n",
    "    base_map = {'A': 0, 'T': 1, 'G': 2, 'C': 3}\n",
    "    for base, idx in base_map.items():\n",
    "        positions = torch.tensor([i for i, b in enumerate(seq) if b == base], \n",
    "                               device=device)\n",
    "        if len(positions) > 0:\n",
    "            tensor[positions, idx] = 1\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "def gpu_sequence_similarity_optimized(seq1, seq2, device='cuda', batch_size=1000):\n",
    "    \"\"\"Calculate sequence similarity using batched GPU operations\"\"\"\n",
    "    try:\n",
    "        # Convert sequences to tensors\n",
    "        with torch.cuda.device(device):\n",
    "            tensor1 = optimized_seq_to_tensor(seq1, device)\n",
    "            tensor2 = optimized_seq_to_tensor(seq2, device)\n",
    "            \n",
    "            # Use batched processing for large sequences\n",
    "            similarity = torch.zeros((len(seq1), len(seq2)), device=device)\n",
    "            for i in range(0, len(seq1), batch_size):\n",
    "                end_i = min(i + batch_size, len(seq1))\n",
    "                for j in range(0, len(seq2), batch_size):\n",
    "                    end_j = min(j + batch_size, len(seq2))\n",
    "                    \n",
    "                    # Process batch\n",
    "                    batch_sim = torch.matmul(\n",
    "                        tensor1[i:end_i], \n",
    "                        tensor2[j:end_j].T\n",
    "                    )\n",
    "                    similarity[i:end_i, j:end_j] = batch_sim\n",
    "                    \n",
    "                    # Synchronize to ensure completion\n",
    "                    torch.cuda.synchronize()\n",
    "            \n",
    "            return similarity.cpu().numpy()\n",
    "    \n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        print(\"GPU OUT OF MEMORY: Trying with smaller batch size...\")\n",
    "        if batch_size > 100:\n",
    "            return gpu_sequence_similarity_optimized(seq1, seq2, device, batch_size//2)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "# Test with different sequence lengths\n",
    "sequence_lengths = [1000, 10000, 100000]\n",
    "print(\"Performance comparison at different sequence lengths:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for length in sequence_lengths:\n",
    "    test_seq1 = \"ATGCTAGCTAGCTGATCG\" * (length // 18 + 1)\n",
    "    test_seq2 = \"GCTAGCTAGCTAGCTAGT\" * (length // 18 + 1)\n",
    "    test_seq1 = test_seq1[:length]\n",
    "    test_seq2 = test_seq2[:length]\n",
    "    \n",
    "    print(f\"\\nTesting with sequence length: {length}\")\n",
    "    \n",
    "    try:\n",
    "        # Time CPU version\n",
    "        torch.cuda.synchronize()  # Ensure GPU is synchronized before timing\n",
    "        start_time = time.time()\n",
    "        cpu_result = gpu_sequence_similarity_optimized(test_seq1, test_seq2, device='cpu')\n",
    "        cpu_time = time.time() - start_time\n",
    "        \n",
    "        # Clear GPU cache\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Time GPU version\n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.time()\n",
    "        gpu_result = gpu_sequence_similarity_optimized(test_seq1, test_seq2, device='cuda')\n",
    "        torch.cuda.synchronize()\n",
    "        gpu_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"CPU time: {cpu_time:.4f} seconds\")\n",
    "        print(f\"GPU time: {gpu_time:.4f} seconds\")\n",
    "        print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n",
    "        \n",
    "        # Verify results match\n",
    "        if np.allclose(cpu_result, gpu_result, rtol=1e-4, atol=1e-4):\n",
    "            print(\"✓ Results match between CPU and GPU\")\n",
    "        else:\n",
    "            print(\"⚠ Warning: Results differ between CPU and GPU!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error at length {length}: {str(e)}\")\n",
    "    \n",
    "    # Clear GPU memory after each test\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742f56da",
   "metadata": {},
   "source": [
    "# GPU-Accelerated Genetics Analysis\n",
    "\n",
    "This notebook demonstrates how to leverage GPU acceleration for genomics tasks using PyTorch and CUDA. We'll start by verifying our GPU setup and then implement several GPU-accelerated genomics algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec554f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.3\n",
      "PyTorch version: 2.9.0a0+50eac811a6.nv25.09\n",
      "CUDA available: True\n",
      "CUDA version: 13.0\n",
      "GPU device: NVIDIA GeForce RTX 5070 Ti\n",
      "Number of GPUs: 1\n",
      "\n",
      "GPU test successful: Matrix multiplication completed\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability and setup\n",
    "import torch\n",
    "import platform\n",
    "\n",
    "print(f\"Python version: {platform.python_version()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    \n",
    "    # Test GPU with a simple operation\n",
    "    x = torch.randn(1000, 1000).cuda()\n",
    "    y = torch.matmul(x, x)\n",
    "    print(\"\\nGPU test successful: Matrix multiplication completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc370078",
   "metadata": {},
   "source": [
    "## DNA Sequence Analysis with GPU Acceleration\n",
    "\n",
    "First, let's create some basic functions for GPU-accelerated DNA sequence analysis. We'll use PyTorch's tensor operations for efficient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff43525f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Similarity Comparison:\n",
      "CPU time: 0.1771 seconds\n",
      "GPU time: 1.6235 seconds\n",
      "Speedup: 0.11x\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "import time\n",
    "\n",
    "def seq_to_tensor(seq, device='cuda'):\n",
    "    \"\"\"Convert a DNA sequence to one-hot encoded tensor on GPU\"\"\"\n",
    "    # One-hot encoding: A=[1,0,0,0], T=[0,1,0,0], G=[0,0,1,0], C=[0,0,0,1]\n",
    "    base_to_idx = {'A': 0, 'T': 1, 'G': 2, 'C': 3}\n",
    "    seq = seq.upper()\n",
    "    \n",
    "    # Create tensor on specified device\n",
    "    one_hot = torch.zeros(len(seq), 4, device=device)\n",
    "    for i, base in enumerate(seq):\n",
    "        if base in base_to_idx:\n",
    "            one_hot[i, base_to_idx[base]] = 1\n",
    "    return one_hot\n",
    "\n",
    "def gpu_sequence_similarity(seq1, seq2, device='cuda'):\n",
    "    \"\"\"Calculate sequence similarity using GPU acceleration\"\"\"\n",
    "    # Convert sequences to one-hot encoded tensors\n",
    "    tensor1 = seq_to_tensor(seq1, device)\n",
    "    tensor2 = seq_to_tensor(seq2, device)\n",
    "    \n",
    "    # Calculate similarity score using matrix multiplication\n",
    "    similarity = torch.matmul(tensor1, tensor2.T)\n",
    "    return similarity.cpu().numpy()\n",
    "\n",
    "# Test the functions\n",
    "test_seq1 = \"ATGCTAGCTAGCTGATCG\" * 1000  # Make sequence longer for better GPU utilization\n",
    "test_seq2 = \"GCTAGCTAGCTAGCTAGT\" * 1000\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Time CPU version\n",
    "    start_time = time.time()\n",
    "    cpu_result = gpu_sequence_similarity(test_seq1, test_seq2, device='cpu')\n",
    "    cpu_time = time.time() - start_time\n",
    "    \n",
    "    # Time GPU version\n",
    "    start_time = time.time()\n",
    "    gpu_result = gpu_sequence_similarity(test_seq1, test_seq2, device='cuda')\n",
    "    gpu_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Sequence Similarity Comparison:\")\n",
    "    print(f\"CPU time: {cpu_time:.4f} seconds\")\n",
    "    print(f\"GPU time: {gpu_time:.4f} seconds\")\n",
    "    print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33485639",
   "metadata": {},
   "source": [
    "## GPU-Accelerated k-mer Analysis\n",
    "\n",
    "Next, let's implement k-mer counting using GPU acceleration. This is particularly useful for analyzing large genomic sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ba4a329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-mer Analysis (k=5):\n",
      "CPU time: 0.0217 seconds\n",
      "GPU time: 0.0336 seconds\n",
      "Speedup: 0.65x\n",
      "\n",
      "Top 5 k-mers in first sequence:\n",
      "TAGCT: 2000\n",
      "GCTAG: 2000\n",
      "CTAGC: 2000\n",
      "ATGCT: 1000\n",
      "AGCTA: 1000\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def generate_kmers(seq, k):\n",
    "    \"\"\"Generate all k-mers from a sequence\"\"\"\n",
    "    return [seq[i:i+k] for i in range(len(seq)-k+1)]\n",
    "\n",
    "def gpu_kmer_frequency(sequences, k=5, device='cuda'):\n",
    "    \"\"\"Calculate k-mer frequencies using GPU acceleration\"\"\"\n",
    "    # Generate all possible k-mers\n",
    "    bases = ['A', 'T', 'G', 'C']\n",
    "    all_kmers = [''.join(p) for p in itertools.product(bases, repeat=k)]\n",
    "    kmer_to_idx = {kmer: idx for idx, kmer in enumerate(all_kmers)}\n",
    "    \n",
    "    # Initialize frequency tensor on GPU\n",
    "    freq_tensor = torch.zeros(len(sequences), len(all_kmers), device=device)\n",
    "    \n",
    "    # Process each sequence\n",
    "    for i, seq in enumerate(sequences):\n",
    "        kmers = generate_kmers(seq, k)\n",
    "        # Convert k-mers to indices\n",
    "        indices = torch.tensor([kmer_to_idx[kmer] for kmer in kmers if kmer in kmer_to_idx], \n",
    "                             device=device)\n",
    "        # Count frequencies using GPU operations\n",
    "        freq_tensor[i].scatter_add_(0, indices, \n",
    "                                  torch.ones(len(indices), device=device))\n",
    "    \n",
    "    return freq_tensor.cpu().numpy(), all_kmers\n",
    "\n",
    "# Test with sample sequences\n",
    "if torch.cuda.is_available():\n",
    "    test_sequences = [\n",
    "        \"ATGCTAGCTAGCTGATCG\" * 1000,\n",
    "        \"GCTAGCTAGCTAGCTAGT\" * 1000\n",
    "    ]\n",
    "    \n",
    "    # Time CPU version\n",
    "    start_time = time.time()\n",
    "    with torch.device('cpu'):\n",
    "        cpu_freq, kmers = gpu_kmer_frequency(test_sequences, k=5, device='cpu')\n",
    "    cpu_time = time.time() - start_time\n",
    "    \n",
    "    # Time GPU version\n",
    "    start_time = time.time()\n",
    "    gpu_freq, kmers = gpu_kmer_frequency(test_sequences, k=5, device='cuda')\n",
    "    gpu_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"K-mer Analysis (k=5):\")\n",
    "    print(f\"CPU time: {cpu_time:.4f} seconds\")\n",
    "    print(f\"GPU time: {gpu_time:.4f} seconds\")\n",
    "    print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n",
    "    \n",
    "    # Show top k-mers for first sequence\n",
    "    top_kmers = sorted(zip(kmers, gpu_freq[0]), key=lambda x: x[1], reverse=True)[:5]\n",
    "    print(\"\\nTop 5 k-mers in first sequence:\")\n",
    "    for kmer, freq in top_kmers:\n",
    "        print(f\"{kmer}: {int(freq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f33e689",
   "metadata": {},
   "source": [
    "# GPU-Accelerated Genetics Analysis\n",
    "\n",
    "This notebook demonstrates how to leverage GPU acceleration for genomics tasks using PyTorch and CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7f25622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 0.1283 seconds\n",
      "GPU time: 0.5078 seconds\n",
      "Speedup: 0.25x\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "import time\n",
    "\n",
    "# Helper function to convert DNA sequence to tensor\n",
    "def seq_to_tensor(seq):\n",
    "    # One-hot encoding: A=[1,0,0,0], T=[0,1,0,0], G=[0,0,1,0], C=[0,0,0,1]\n",
    "    base_to_idx = {'A': 0, 'T': 1, 'G': 2, 'C': 3}\n",
    "    seq_indices = [base_to_idx[base] for base in seq.upper()]\n",
    "    one_hot = torch.zeros(len(seq), 4)\n",
    "    one_hot[range(len(seq)), seq_indices] = 1\n",
    "    return one_hot\n",
    "\n",
    "# GPU-accelerated sequence similarity scoring\n",
    "def gpu_sequence_similarity(seq1, seq2, device='cuda'):\n",
    "    # Convert sequences to one-hot encoded tensors\n",
    "    tensor1 = seq_to_tensor(seq1).to(device)\n",
    "    tensor2 = seq_to_tensor(seq2).to(device)\n",
    "    \n",
    "    # Calculate similarity score using matrix multiplication\n",
    "    similarity = torch.matmul(tensor1, tensor2.T)\n",
    "    return similarity.cpu().numpy()\n",
    "\n",
    "# Test GPU acceleration\n",
    "test_seq1 = \"ATGCTAGCTAGCTGATCG\" * 1000  # Make sequence longer for better GPU utilization\n",
    "test_seq2 = \"GCTAGCTAGCTAGCTAGT\" * 1000\n",
    "\n",
    "# Time CPU version\n",
    "start_time = time.time()\n",
    "cpu_result = gpu_sequence_similarity(test_seq1, test_seq2, device='cpu')\n",
    "cpu_time = time.time() - start_time\n",
    "\n",
    "# Time GPU version\n",
    "if torch.cuda.is_available():\n",
    "    start_time = time.time()\n",
    "    gpu_result = gpu_sequence_similarity(test_seq1, test_seq2, device='cuda')\n",
    "    gpu_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"CPU time: {cpu_time:.4f} seconds\")\n",
    "    print(f\"GPU time: {gpu_time:.4f} seconds\")\n",
    "    print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n",
    "else:\n",
    "    print(\"CUDA not available. Please ensure GPU support is properly configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d8ce6",
   "metadata": {},
   "source": [
    "## GPU-Accelerated Multiple Sequence Alignment\n",
    "\n",
    "Here's an example of using GPU acceleration for multiple sequence alignment scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b54b50a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Sequence Alignment:\n",
      "CPU time: 0.0125 seconds\n",
      "GPU time: 0.0184 seconds\n",
      "Speedup: 0.68x\n",
      "\n",
      "Alignment Score Matrix:\n",
      "[[     0. 820000. 820000. 810000.]\n",
      " [820000.      0. 820000. 810000.]\n",
      " [820000. 820000.      0. 810000.]\n",
      " [810000. 810000. 810000.      0.]]\n"
     ]
    }
   ],
   "source": [
    "def gpu_multiple_sequence_alignment(sequences, device='cuda'):\n",
    "    \"\"\"\n",
    "    Compute pairwise alignment scores for multiple sequences using GPU acceleration\n",
    "    \"\"\"\n",
    "    n_sequences = len(sequences)\n",
    "    score_matrix = torch.zeros((n_sequences, n_sequences), device=device)\n",
    "    \n",
    "    # Convert all sequences to tensors first\n",
    "    seq_tensors = [seq_to_tensor(seq).to(device) for seq in sequences]\n",
    "    \n",
    "    # Compute all pairwise scores in parallel\n",
    "    for i in range(n_sequences):\n",
    "        for j in range(i+1, n_sequences):\n",
    "            score = torch.sum(torch.matmul(seq_tensors[i], seq_tensors[j].T))\n",
    "            score_matrix[i,j] = score\n",
    "            score_matrix[j,i] = score\n",
    "    \n",
    "    return score_matrix.cpu().numpy()\n",
    "\n",
    "# Test with multiple sequences\n",
    "test_sequences = [\n",
    "    \"ATGCTAGCTAGCTGATCG\" * 100,\n",
    "    \"GCTAGCTAGCTAGCTAGT\" * 100,\n",
    "    \"TAGCTAGCTAGCTGATCG\" * 100,\n",
    "    \"ATGCTAGCTAGCTGATCC\" * 100\n",
    "]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Time CPU version\n",
    "    start_time = time.time()\n",
    "    cpu_scores = gpu_multiple_sequence_alignment(test_sequences, device='cpu')\n",
    "    cpu_time = time.time() - start_time\n",
    "    \n",
    "    # Time GPU version\n",
    "    start_time = time.time()\n",
    "    gpu_scores = gpu_multiple_sequence_alignment(test_sequences, device='cuda')\n",
    "    gpu_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Multiple Sequence Alignment:\")\n",
    "    print(f\"CPU time: {cpu_time:.4f} seconds\")\n",
    "    print(f\"GPU time: {gpu_time:.4f} seconds\")\n",
    "    print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n",
    "    print(\"\\nAlignment Score Matrix:\")\n",
    "    print(gpu_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dc85cd",
   "metadata": {},
   "source": [
    "## GPU-Accelerated k-mer Analysis\n",
    "\n",
    "Implementing k-mer counting using GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1871cc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-mer Analysis (k=5):\n",
      "CPU time: 0.0064 seconds\n",
      "GPU time: 0.0065 seconds\n",
      "Speedup: 0.98x\n",
      "\n",
      "Top 5 k-mers in first sequence:\n",
      "TAGCT: 2000\n",
      "GCTAG: 2000\n",
      "CTAGC: 2000\n",
      "ATGCT: 1000\n",
      "AGCTA: 1000\n"
     ]
    }
   ],
   "source": [
    "def generate_kmers(seq, k):\n",
    "    \"\"\"Generate all k-mers from a sequence\"\"\"\n",
    "    return [seq[i:i+k] for i in range(len(seq)-k+1)]\n",
    "\n",
    "def gpu_kmer_frequency(sequences, k=5, device='cuda'):\n",
    "    \"\"\"\n",
    "    Calculate k-mer frequencies using GPU acceleration\n",
    "    \"\"\"\n",
    "    # Generate all possible k-mers\n",
    "    bases = ['A', 'T', 'G', 'C']\n",
    "    all_kmers = [''.join(p) for p in itertools.product(bases, repeat=k)]\n",
    "    kmer_to_idx = {kmer: idx for idx, kmer in enumerate(all_kmers)}\n",
    "    \n",
    "    # Initialize frequency tensor on GPU\n",
    "    freq_tensor = torch.zeros(len(sequences), len(all_kmers), device=device)\n",
    "    \n",
    "    # Process each sequence\n",
    "    for i, seq in enumerate(sequences):\n",
    "        kmers = generate_kmers(seq, k)\n",
    "        # Convert k-mers to indices\n",
    "        indices = torch.tensor([kmer_to_idx[kmer] for kmer in kmers if kmer in kmer_to_idx], \n",
    "                             device=device)\n",
    "        # Count frequencies using GPU operations\n",
    "        freq_tensor[i].scatter_add_(0, indices, \n",
    "                                  torch.ones(len(indices), device=device))\n",
    "    \n",
    "    return freq_tensor.cpu().numpy(), all_kmers\n",
    "\n",
    "# Test with sample sequences\n",
    "if torch.cuda.is_available():\n",
    "    import itertools\n",
    "    \n",
    "    test_sequences = [\n",
    "        \"ATGCTAGCTAGCTGATCG\" * 1000,\n",
    "        \"GCTAGCTAGCTAGCTAGT\" * 1000\n",
    "    ]\n",
    "    \n",
    "    # Time CPU version\n",
    "    start_time = time.time()\n",
    "    with torch.device('cpu'):\n",
    "        cpu_freq, kmers = gpu_kmer_frequency(test_sequences, k=5, device='cpu')\n",
    "    cpu_time = time.time() - start_time\n",
    "    \n",
    "    # Time GPU version\n",
    "    start_time = time.time()\n",
    "    gpu_freq, kmers = gpu_kmer_frequency(test_sequences, k=5, device='cuda')\n",
    "    gpu_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"K-mer Analysis (k=5):\")\n",
    "    print(f\"CPU time: {cpu_time:.4f} seconds\")\n",
    "    print(f\"GPU time: {gpu_time:.4f} seconds\")\n",
    "    print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n",
    "    \n",
    "    # Show top k-mers for first sequence\n",
    "    top_kmers = sorted(zip(kmers, gpu_freq[0]), key=lambda x: x[1], reverse=True)[:5]\n",
    "    print(\"\\nTop 5 k-mers in first sequence:\")\n",
    "    for kmer, freq in top_kmers:\n",
    "        print(f\"{kmer}: {int(freq)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582aff51-e595-4ac4-8425-4162e90b279e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
