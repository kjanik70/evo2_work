{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52b09c43",
   "metadata": {},
   "source": [
    "# EVO2 Predictions on Genome FASTA\n",
    "\n",
    "This notebook reads FASTA file(s) in the `data/` directory, prepares sequences with lengths from 128 up to 1,048,576 (doubling each step), sends each sequence to the EVO2 model to generate 128 characters, and compares the generated 128 characters to the next 128 characters from the FASTA genome.\n",
    "\n",
    "Notes:\n",
    "- Set the `NVCF_RUN_KEY` environment variable in your container or paste it when prompted.\n",
    "- The example uses the NIM HTTP endpoint; be mindful of rate limits and cost.\n",
    "- If the FASTA genome is shorter than required sizes, the genome will be repeated (wrap) to create the required length for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc6d7e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIM_EVO2_TIMEOUT_S in notebook: 600\n",
      "NIM_EVO2_TIMEOUT_S in notebook: 600\n"
     ]
    }
   ],
   "source": [
    "# Imports and configuration\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from Bio import SeqIO\n",
    "\n",
    "# API configuration\n",
    "KEY = os.getenv(\"NVCF_RUN_KEY\") or input(\"Paste the Run Key (NVCF_RUN_KEY): \")\n",
    "URL = os.getenv(\"URL\", \"https://health.api.nvidia.com/v1/biology/arc/evo2-40b/generate\")\n",
    "HEADERS = {\"Authorization\": f\"Bearer {KEY}\"}\n",
    "\n",
    "# Paths\n",
    "WORKSPACE = Path('/workspace') if Path('/workspace').exists() else Path('.')\n",
    "DATA_DIR = WORKSPACE / 'data'\n",
    "RESULTS_DIR = WORKSPACE / 'results'\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Print to confirm\n",
    "print(\"NIM_EVO2_TIMEOUT_S in notebook:\", os.environ.get(\"NIM_EVO2_TIMEOUT_S\"))\n",
    "os.environ[\"NIM_EVO2_TIMEOUT_S\"] = \"600\"\n",
    "print(\"NIM_EVO2_TIMEOUT_S in notebook:\", os.environ.get(\"NIM_EVO2_TIMEOUT_S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e6ddbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded genome length: 119668634 from 1 fasta file(s)\n"
     ]
    }
   ],
   "source": [
    "# Load FASTA(s) from data directory and concatenate all sequences into a single genome string\n",
    "fasta_paths = list(DATA_DIR.glob('*.fasta')) + list(DATA_DIR.glob('*.fa'))\n",
    "if not fasta_paths:\n",
    "    raise FileNotFoundError(f'No FASTA files found in {DATA_DIR.resolve()}')\n",
    "\n",
    "records = []\n",
    "for p in fasta_paths:\n",
    "    for rec in SeqIO.parse(str(p), 'fasta'):\n",
    "        records.append(str(rec.seq))\n",
    "\n",
    "genome = ''.join(records).upper()\n",
    "print(f'Loaded genome length: {len(genome)} from {len(fasta_paths)} fasta file(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e722a407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: call EVO2 model via NIM HTTP endpoint\n",
    "import json\n",
    "\n",
    "def call_evo2(sequence, num_tokens=128, top_k=1, enable_sampled_probs=True, url=URL, headers=HEADERS, timeout=120):\n",
    "    \"\"\"Call the EVO2 NIM HTTP endpoint with extra debug logging on errors.\n",
    "\n",
    "    Returns parsed JSON on 2xx responses, otherwise prints diagnostics and returns None.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        'sequence': sequence,\n",
    "        'num_tokens': num_tokens,\n",
    "        'top_k': top_k,\n",
    "        'enable_sampled_probs': enable_sampled_probs,\n",
    "    }\n",
    "    try:\n",
    "        print(f\"[call_evo2] seq_len={len(sequence)}, num_tokens={num_tokens}, payload_keys={list(payload.keys())}\")\n",
    "        r = requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "        # If non-2xx, show status and body for debugging\n",
    "        if not (200 <= r.status_code < 300):\n",
    "            body = r.text\n",
    "            printed = body[:2000] if isinstance(body, str) else str(body)\n",
    "            print(f\"[call_evo2] HTTP {r.status_code} response body (truncated 2000 chars):\\n{printed}\")\n",
    "            # raise to let caller handle as well\n",
    "            r.raise_for_status()\n",
    "        # parse json if possible\n",
    "        try:\n",
    "            return r.json()\n",
    "        except ValueError:\n",
    "            # not JSON; return text under a key so caller can still inspect\n",
    "            print('[call_evo2] Response not JSON; returning text in dict')\n",
    "            return {'text': r.text}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print('Request error:', e)\n",
    "        return None\n",
    "\n",
    "# Helper: compare predicted string to truth string\n",
    "def compare_sequences(pred, truth):\n",
    "    # normalize and remove literal newline characters using chr(10) to avoid JSON escaping issues\n",
    "    pred = (pred or '').strip().replace(chr(10), '')\n",
    "    truth = (truth or '').strip().replace(chr(10), '')\n",
    "    n = min(len(pred), len(truth))\n",
    "    if n == 0:\n",
    "        return {'length': len(pred), 'matches': 0, 'accuracy': 0.0}\n",
    "    matches = sum(1 for i in range(n) if pred[i] == truth[i])\n",
    "    return {'length': n, 'matches': matches, 'accuracy': matches / n}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34ef0893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking helper\n",
    "from typing import Iterator\n",
    "\n",
    "def chunk_sequence(seq: str, chunk_size: int) -> Iterator[str]:\n",
    "    \"\"\"Yield successive chunks of `seq` with size <= chunk_size.\n",
    "\n",
    "    Example:\n",
    "        list(chunk_sequence('ABCDEFG', 3)) -> ['ABC', 'DEF', 'G']\n",
    "    \"\"\"\n",
    "    if chunk_size <= 0:\n",
    "        raise ValueError('chunk_size must be > 0')\n",
    "    for i in range(0, len(seq), chunk_size):\n",
    "        yield seq[i:i+chunk_size]\n",
    "\n",
    "# Quick non-network test helper\n",
    "def _debug_show_chunks(seq: str, chunk_size: int):\n",
    "    ch = list(chunk_sequence(seq, chunk_size))\n",
    "    print('num_chunks=', len(ch))\n",
    "    print('chunk lengths=', [len(c) for c in ch])\n",
    "    return ch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28bdd157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encapsulated runner with explicit chunking and retries\n",
    "import itertools\n",
    "\n",
    "def run_evo2_tests(max_chunk=2048, sizes=None, start_index=0, out_csv_path=None, sleep_between=1, max_retries=2):\n",
    "    \"\"\"Run EVO2 predictions for sizes and handle chunking/retries.\n",
    "\n",
    "    - max_chunk: chunk size to split input into when the test size exceeds this value.\n",
    "    - sizes: list of sizes to test (if None, uses powers of two from 128..1,048,576).\n",
    "    - start_index: offset into the genome to start each test.\n",
    "    - out_csv_path: Path object or string for results CSV (default uses RESULTS_DIR).\n",
    "    - sleep_between: seconds to sleep between requests (default 1s).\n",
    "    - max_retries: number of tries per chunk/request before giving up.\n",
    "    \"\"\"\n",
    "    if sizes is None:\n",
    "        sizes = []\n",
    "        L = 128\n",
    "        while L <= 1048576:\n",
    "            sizes.append(L)\n",
    "            L *= 2\n",
    "    print(f\"Running tests for sizes: {sizes}\")\n",
    "\n",
    "    if out_csv_path is None:\n",
    "        out_csv_path = RESULTS_DIR / 'evo2_predictions_results.csv'\n",
    "    else:\n",
    "        out_csv_path = Path(out_csv_path)\n",
    "\n",
    "    # prepare CSV\n",
    "    with open(out_csv_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=['size_sent','pred_len','matches','accuracy','note'])\n",
    "        writer.writeheader()\n",
    "\n",
    "    def safe_call(seq, num_tokens=128):\n",
    "        \"\"\"Call EVO2 with retries.\"\"\"\n",
    "        last_exc = None\n",
    "        for attempt in range(1, max_retries+1):\n",
    "            try:\n",
    "                resp = call_evo2(seq, num_tokens=num_tokens)\n",
    "                if resp is None:\n",
    "                    raise RuntimeError('Empty/None response')\n",
    "                return resp\n",
    "            except Exception as e:\n",
    "                print(f\"  Attempt {attempt}/{max_retries} failed: {e}\")\n",
    "                last_exc = e\n",
    "                time.sleep(0.5 * attempt)\n",
    "        print(\"  All retries failed for this chunk.\")\n",
    "        raise last_exc\n",
    "\n",
    "    for size in sizes:\n",
    "        needed = size + 128\n",
    "        if len(genome) < needed:\n",
    "            repeats = (needed // len(genome)) + 2\n",
    "            biggenome = (genome * repeats)\n",
    "        else:\n",
    "            biggenome = genome\n",
    "\n",
    "        seq_to_send = biggenome[start_index:start_index + size]\n",
    "        truth_seq = biggenome[start_index + size:start_index + size + 128]\n",
    "\n",
    "        print(f'--- Size {size}: sending sequence length {len(seq_to_send)} to model')\n",
    "\n",
    "        pred = ''\n",
    "        note = ''\n",
    "        # chunking logic\n",
    "        if len(seq_to_send) >= max_chunk:\n",
    "            chunks = list(chunk_sequence(seq_to_send, max_chunk))\n",
    "            print(f'Chunking enabled. {len(chunks)} chunks; chunk_size={max_chunk}; chunk lengths: {[len(c) for c in chunks]}')\n",
    "            generated_full = ''\n",
    "            try:\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    print(f'  -> Sending chunk {i+1}/{len(chunks)} (len={len(chunk)})')\n",
    "                    resp = safe_call(chunk, num_tokens=128)\n",
    "                    # extract string\n",
    "                    generated = ''\n",
    "                    if isinstance(resp, dict):\n",
    "                        for key in ('sequence','generated_sequence','generated','text'):\n",
    "                            if key in resp and isinstance(resp[key], str):\n",
    "                                generated = resp[key]\n",
    "                                break\n",
    "                        if not generated:\n",
    "                            def find_string(d):\n",
    "                                if isinstance(d, str):\n",
    "                                    return d\n",
    "                                if isinstance(d, dict):\n",
    "                                    for v in d.values():\n",
    "                                        s = find_string(v)\n",
    "                                        if s:\n",
    "                                            return s\n",
    "                                if isinstance(d, list):\n",
    "                                    for v in d:\n",
    "                                        s = find_string(v)\n",
    "                                        if s:\n",
    "                                            return s\n",
    "                                return None\n",
    "                            found = find_string(resp)\n",
    "                            if found:\n",
    "                                generated = found\n",
    "                    part = (generated or '').replace(chr(10), '')\n",
    "                    print(f'    received {len(part)} generated chars from chunk {i+1}')\n",
    "                    generated_full += part\n",
    "                    time.sleep(sleep_between)\n",
    "                pred = generated_full[:128]\n",
    "            except Exception as e:\n",
    "                note = 'chunk_failed'\n",
    "                print('Chunked requests failed:', e)\n",
    "                with open(out_csv_path, 'a', newline='') as csvfile:\n",
    "                    writer = csv.DictWriter(csvfile, fieldnames=['size_sent','pred_len','matches','accuracy','note'])\n",
    "                    writer.writerow({'size_sent': size, 'pred_len': 0, 'matches': 0, 'accuracy': 0.0, 'note': note})\n",
    "                continue\n",
    "        else:\n",
    "            print('No chunking needed for this size')\n",
    "            try:\n",
    "                resp = safe_call(seq_to_send, num_tokens=128)\n",
    "            except Exception as e:\n",
    "                note = 'request_failed'\n",
    "                print('Request failed:', e)\n",
    "                with open(out_csv_path, 'a', newline='') as csvfile:\n",
    "                    writer = csv.DictWriter(csvfile, fieldnames=['size_sent','pred_len','matches','accuracy','note'])\n",
    "                    writer.writerow({'size_sent': size, 'pred_len': 0, 'matches': 0, 'accuracy': 0.0, 'note': note})\n",
    "                continue\n",
    "            generated = ''\n",
    "            if isinstance(resp, dict):\n",
    "                for key in ('sequence','generated_sequence','generated','text'):\n",
    "                    if key in resp and isinstance(resp[key], str):\n",
    "                        generated = resp[key]\n",
    "                        break\n",
    "                if not generated:\n",
    "                    def find_string(d):\n",
    "                        if isinstance(d, str):\n",
    "                            return d\n",
    "                        if isinstance(d, dict):\n",
    "                            for v in d.values():\n",
    "                                s = find_string(v)\n",
    "                                if s:\n",
    "                                    return s\n",
    "                        if isinstance(d, list):\n",
    "                            for v in d:\n",
    "                                s = find_string(v)\n",
    "                                if s:\n",
    "                                    return s\n",
    "                        return None\n",
    "                    found = find_string(resp)\n",
    "                    if found:\n",
    "                        generated = found\n",
    "            pred = (generated or '').replace(chr(10), '')[:128]\n",
    "\n",
    "        comp = compare_sequences(pred, truth_seq)\n",
    "        print(\"Size {}: pred_len={}, matches={}, accuracy={:.4f}\".format(size, comp['length'], comp['matches'], comp['accuracy']))\n",
    "\n",
    "        with open(out_csv_path, 'a', newline='') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=['size_sent','pred_len','matches','accuracy','note'])\n",
    "            writer.writerow({'size_sent': size, 'pred_len': comp['length'], 'matches': comp['matches'], 'accuracy': comp['accuracy'], 'note': note})\n",
    "\n",
    "    print('\\nAll tests complete. Results saved to', out_csv_path)\n",
    "\n",
    "# To run the tests, call: run_evo2_tests(max_chunk=2048)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c95ba661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes to test: [256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072, 262144, 524288, 1048576]\n",
      "--- Size 256: sending sequence length 256 to model\n",
      "[call_evo2] seq_len=256, num_tokens=128, payload_keys=['sequence', 'num_tokens', 'top_k', 'enable_sampled_probs']\n",
      "Size 256: pred_len=128, matches=56, accuracy=0.4375\n",
      "--- Size 512: sending sequence length 512 to model\n",
      "[call_evo2] seq_len=512, num_tokens=128, payload_keys=['sequence', 'num_tokens', 'top_k', 'enable_sampled_probs']\n",
      "Size 512: pred_len=128, matches=47, accuracy=0.3672\n",
      "--- Size 1024: sending sequence length 1024 to model\n",
      "[call_evo2] seq_len=1024, num_tokens=128, payload_keys=['sequence', 'num_tokens', 'top_k', 'enable_sampled_probs']\n",
      "Size 1024: pred_len=128, matches=43, accuracy=0.3359\n",
      "--- Size 2048: sending sequence length 2048 to model\n",
      "[call_evo2] seq_len=2048, num_tokens=128, payload_keys=['sequence', 'num_tokens', 'top_k', 'enable_sampled_probs']\n",
      "Size 2048: pred_len=128, matches=62, accuracy=0.4844\n",
      "--- Size 4096: sending sequence length 4096 to model\n",
      "[call_evo2] seq_len=4096, num_tokens=128, payload_keys=['sequence', 'num_tokens', 'top_k', 'enable_sampled_probs']\n",
      "[call_evo2] HTTP 422 response body (truncated 2000 chars):\n",
      "{\"type\":\"urn:inference-service:problem-details:unprocessable-entity\",\"title\":\"Unprocessable Entity\",\"status\":422,\"detail\":\"Timed out on 3101th token. Allowed to run for 120 seconds. You can change the limit by setting NIM_EVO2_TIMEOUT_S environment variable. len(input_string)=4096 num_tokens=128\"}\n",
      "Request error: 422 Client Error: Unprocessable Entity for url: https://health.api.nvidia.com/v1/biology/arc/evo2-40b/generate\n",
      "Request failed, stopping further tests.\n",
      "\n",
      "All tests complete. Results saved to /workspace/results/evo2_predictions_results.csv\n",
      "num_chunks= 2\n",
      "chunk lengths= [2048, 2048]\n"
     ]
    }
   ],
   "source": [
    "# Prepare sizes: start at 128 and double until 1,048,576 (2^20)\n",
    "sizes = []\n",
    "L = 256\n",
    "while L <= 1048576:\n",
    "    sizes.append(L)\n",
    "    L *= 2\n",
    "print('Sizes to test:', sizes)\n",
    "\n",
    "# Main loop: for each size, take the first `size` bases, ask model to generate 128 bases, compare to next 128 bases\n",
    "start_index = 0  # you can modify this to sample different positions\n",
    "OUT_CSV = RESULTS_DIR / 'evo2_predictions_results.csv'\n",
    "\n",
    "# CSV header\n",
    "with open(OUT_CSV, 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=['size_sent','pred_len','matches','accuracy','note'])\n",
    "    writer.writeheader()\n",
    "\n",
    "for size in sizes:\n",
    "    needed = size + 128  # we need the sequence and the following 128 bases to compare against\n",
    "    if len(genome) < needed:\n",
    "        # repeat (wrap) the genome to make enough length\n",
    "        repeats = (needed // len(genome)) + 2\n",
    "        biggenome = (genome * repeats)\n",
    "    else:\n",
    "        biggenome = genome\n",
    "\n",
    "    seq_to_send = biggenome[start_index:start_index + size]\n",
    "    truth_seq = biggenome[start_index + size:start_index + size + 128]\n",
    "\n",
    "    print(f'--- Size {size}: sending sequence length {len(seq_to_send)} to model')\n",
    "\n",
    "    # Call EVO2 - be mindful: large payloads can be slow or restricted by the service\n",
    "    resp = call_evo2(seq_to_send, num_tokens=128)\n",
    "    if resp is None:\n",
    "        note = 'request_failed'\n",
    "        print('Request failed, stopping further tests.')\n",
    "        with open(OUT_CSV, 'a', newline='') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=['size_sent','pred_len','matches','accuracy','note'])\n",
    "            writer.writerow({'size_sent': size, 'pred_len': 0, 'matches': 0, 'accuracy': 0.0, 'note': note})\n",
    "        break\n",
    "\n",
    "    # The response format may vary. Typical key could be 'sequence' or similar. Handle common variants.\n",
    "    generated = ''\n",
    "    if isinstance(resp, dict):\n",
    "        # try common keys\n",
    "        for key in ('sequence','generated_sequence','generated','text'):\n",
    "            if key in resp and isinstance(resp[key], str):\n",
    "                generated = resp[key]\n",
    "                break\n",
    "        # If the API returns nested structures, try to extract first string occurrence\n",
    "        if not generated:\n",
    "            # try to find a string value in the JSON response\n",
    "            def find_string(d):\n",
    "                if isinstance(d, str):\n",
    "                    return d\n",
    "                if isinstance(d, dict):\n",
    "                    for v in d.values():\n",
    "                        s = find_string(v)\n",
    "                        if s:\n",
    "                            return s\n",
    "                if isinstance(d, list):\n",
    "                    for v in d:\n",
    "                        s = find_string(v)\n",
    "                        if s:\n",
    "                            return s\n",
    "                return None\n",
    "            found = find_string(resp)\n",
    "            if found:\n",
    "                generated = found\n",
    "\n",
    "    generated = (generated or '').replace(chr(10), '')\n",
    "    pred = generated[:128]  # only compare the first 128 generated characters\n",
    "\n",
    "    #print(\"Predicted: \",pred)\n",
    "    #print(\"Truth:     \",truth_seq)\n",
    "\n",
    "    comp = compare_sequences(pred, truth_seq)\n",
    "    print(\"Size {}: pred_len={}, matches={}, accuracy={:.4f}\".format(size, comp['length'], comp['matches'], comp['accuracy']))\n",
    "\n",
    "    with open(OUT_CSV, 'a', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=['size_sent','pred_len','matches','accuracy','note'])\n",
    "        writer.writerow({'size_sent': size, 'pred_len': comp['length'], 'matches': comp['matches'], 'accuracy': comp['accuracy'], 'note': ''})\n",
    "\n",
    "    # Gentle delay to avoid rate limits (adjust as needed)\n",
    "    time.sleep(1)\n",
    "\n",
    "print('\\nAll tests complete. Results saved to', OUT_CSV)\n",
    "\n",
    "test_seq = genome[:4096]\n",
    "chunks = list(chunk_sequence(test_seq, 2048))\n",
    "print('num_chunks=', len(chunks))\n",
    "print('chunk lengths=', [len(c) for c in chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd19a574",
   "metadata": {},
   "source": [
    "### Notes & Next steps\n",
    "- The service may enforce limits on request size; if you get errors for large `size`, consider smaller maximum sizes or chunking strategies.\n",
    "- You can randomize `start_index` to sample different genome locations.\n",
    "- For production scale tests, add robust retry/backoff and monitor API quotas/costs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch GPU)",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
